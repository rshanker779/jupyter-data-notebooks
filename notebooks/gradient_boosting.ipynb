{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our problem, have a training set $X$, a test set $y$, and a differentiable error function $E$ which we consider a function $E(f(x_i), y_i)$ for some model $f$ and a training example $x_i \\in X$ and corresponding $y_i\\in y$.\n",
    "So the total error is given by $\\frac{1}{n} \\sum_i E(f(x_i), y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to have an iterative approximation to a perfect function, mimicking gradient descent in the function space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does this look like? We start with some initial model, $f_0$, for example could be the training mean for a regression problem, or majority class output in a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then consider the error for that function. Since $E$ is differentiable, can compute $$r_i = -\\frac{\\partial E(f_0(x_i), y_i)}{\\partial f_0(x_i)}$$, the residuals for each training example. Let's give a conrete example, mean square error.\n",
    "\n",
    "$$E(f(x_i), y_i)  = \\tfrac{1}{2}(f(x_i)-y_i)^2$$\n",
    "\n",
    "We differentiate $E$ considering it a function of $f(x_i)$, so compute \n",
    "\n",
    "$$-\\frac{\\partial E(f(x_i), y_i)}{\\partial f(x_i)} = -(f(x_i)-y_i) = y_i-f(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have computed these residuals for each example in our training set. But we need to extend this a function for every input, so we fit some function to these values. This can be any function, in the case of XGBoost and LightGBM this would be a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we fit some function (a so-called \"weak learner\") to these residuals, and get some $h_0$. Then the last step is to combine this to our next iteration of the model. We take some real number $\\alpha>0$, the learning rate, then compute \n",
    "\n",
    "$$f_1(x_i) = f_0(x_i) + \\alpha h_0(x_i)$$ \n",
    "\n",
    "Note this learning rate can be fixed, but we can also choose it in other ways, for example choose $\\alpha$ that minimizes our total error (this is a one dimensional optimisation problem, so can use variety of solvers that exist to find a solution).\n",
    "\n",
    "The key point is that this look like gradient boosting, as\n",
    "\n",
    "$$ f_0(x_i) + \\alpha h_0(x_i) \\approx  f_0(x_i) - \\alpha \\frac{\\partial E(f_0(x_i), y_i)}{\\partial f_0(x_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we continue- notice we have used no special properties of $f_0$. So do the same thing starting with $f_1$ to get $f_2$, and continue up until some sensible stopping criteria. This criteria could be a fixed number of iterations, or early stopping based on computing the errors on some validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
