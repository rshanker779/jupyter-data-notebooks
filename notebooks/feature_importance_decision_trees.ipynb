{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Feature Importance for Decison Trees and Ensembles of Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With decision trees, there are a few ways feature importance can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25505156, -0.5652961 , -2.17026121, -0.35362786, -0.74074747],\n",
       "       [-0.32393187, -0.90590188, -2.24490763, -0.67502183, -0.13278426],\n",
       "       [ 0.28806083,  0.70623367, -1.02620405,  0.61980106,  1.79116846],\n",
       "       [ 0.61253165, -0.37773326, -1.82401806,  0.17100044, -1.72567135],\n",
       "       [-2.19599106,  1.86300361, -0.64939349,  0.16065854, -0.85898532],\n",
       "       [ 0.76200858,  0.26922756, -0.23828011, -0.20642094,  0.48842647],\n",
       "       [-1.46191431, -1.69842329,  2.41649117, -0.83833097,  0.38116374],\n",
       "       [-3.16147408, -0.33172241,  0.66478335, -0.99090328,  1.01788005],\n",
       "       [ 2.03945994,  1.85498036,  1.84218187,  0.3415874 , -1.25088622],\n",
       "       [ 2.9519817 ,  2.20047613,  0.91542817,  0.92525075, -0.90478616]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=10,\n",
    "                           n_features=5,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=0,\n",
    "                           shuffle=False,\n",
    "                           )\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make some random data for a binary classification, 10 samples with 5 features, of which 3 are informative.\n",
    "We will then train a model. This same method of calculating feature importance would work for a single decision tree, or an ensemble method, such as Bagging, Random Forests or Gradient Boosting. Here we'll use Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion='gini',random_state=0, n_estimators=5).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit a tree using Gini. Scikit learn supports either Gini, or Entropy. With scikit learn's default feature importance, the criterion used to fit the tree is the same criterion used to evaluate feature importance, but in theory this isn't necessary. It is possible to fit a tree using one metric, and evaluate feature importances with another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini metric (or impurity) for binary classification is defined as follows- suppose at a leaf of a tree we have $a$ samples of one class, and $b$ of the other, where $a>b$. Then the probability we would get from that leaf is a probability $\\frac{a}{a+b}$ of majority class, and $\\frac{b}{a+b}$ of the other. The Gini metric is defined as \n",
    "\n",
    "$$\\frac{1}{a+b}\\sum_i P(\\text{Incorrect prediction of item } i)$$ \n",
    "\n",
    "So in our set up, for $b$ of the items, the probabililty of missclassification is $\\frac{a}{a+b}$, and for $a$ it is $\\frac{b}{a+b}$, giving $$\\frac{2ab}{(a+b)^2}  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.625 0.375 0.    0.    0.   ]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"265pt\" height=\"269pt\"\n",
       " viewBox=\"0.00 0.00 265.00 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-265 261,-265 261,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"204.5,-261 107.5,-261 107.5,-193 204.5,-193 204.5,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[1] &lt;= 1.453</text>\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.48</text>\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-215.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6</text>\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [6, 4]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"147,-157 55,-157 55,-89 147,-89 147,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"101\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[0] &lt;= 0.45</text>\n",
       "<text text-anchor=\"middle\" x=\"101\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.375</text>\n",
       "<text text-anchor=\"middle\" x=\"101\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"101\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [6, 2]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M137.9909,-192.9465C133.4801,-184.4169 128.5809,-175.153 123.8686,-166.2424\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"126.8185,-164.3337 119.0495,-157.13 120.6306,-167.6062 126.8185,-164.3337\"/>\n",
       "<text text-anchor=\"middle\" x=\"111.6868\" y=\"-177.3212\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"257,-149.5 165,-149.5 165,-96.5 257,-96.5 257,-149.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"211\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"211\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"211\" y=\"-104.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M174.0091,-192.9465C179.8342,-181.9316 186.307,-169.6922 192.1888,-158.5703\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"195.3822,-160.0184 196.9633,-149.5422 189.1943,-156.7459 195.3822,-160.0184\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.326\" y=\"-169.7334\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"92,-53 0,-53 0,0 92,0 92,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4</text>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [6, 0]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M81.6091,-88.9777C76.651,-80.2786 71.3067,-70.9018 66.3042,-62.1247\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"69.2378,-60.2033 61.2452,-53.2485 63.1562,-63.6696 69.2378,-60.2033\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"202,-53 110,-53 110,0 202,0 202,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"156\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M120.3909,-88.9777C125.349,-80.2786 130.6933,-70.9018 135.6958,-62.1247\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"138.8438,-63.6696 140.7548,-53.2485 132.7622,-60.2033 138.8438,-63.6696\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f89d65a8198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualise_estimator(model, index):\n",
    "    tree = model.estimators_[index]\n",
    "    print(tree.feature_importances_)\n",
    "    return graphviz.Source(export_graphviz(tree))\n",
    "visualise_estimator(rf, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first decision tree that has been trained by our random forest. For the first split, we have 6 samples of one class, 4 of the other. So for 6 samples we missclassify with probability $0.4$, for 4 categories with probability $0.6$, giving us a Gini of $\\tfrac{1}{10}(6 \\cdot 0.4 + 4 \\cdot0.6) = 0.48$ as shown at that split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini at the other splits can be worked out similarly. For feature importance, mean decrease in impurity is used. So to evaluate the splits, this formula is used \n",
    "$$\\frac{\\text{Data in split}}{\\text{Total data}} \\left( \\text{Data fraction in left} \\cdot \\text{Decrease in metric in left} + \\text{Data fraction in right} \\cdot \\text{Decrease in metric in right} \\right) $$\n",
    "\n",
    "That's quite long winded, but an example should make it clearer. Let's evaluate the first split. So this split separates all 10 if our training cases. The left split has 8 samples, and the right split the other 2. On the left side, the decrease in metric is $0.48-0.375$, and on the right it is $0.48-0$. So the total evaluation of that split is $$\\frac{10}{10} (0.8 \\cdot  (0.48-0.375) + 0.2 \\cdot(0.48-0))=0.18$$\n",
    "\n",
    "In a similar fashion the second split has a score $$\\frac{8}{10} (0.75\\cdot(0.375-0)+0.25\\cdot(0.375))=0.3$$\n",
    "\n",
    "This is all the splits on our tree, but for a large tree we would work out this value for every split. Then for each feature, would sum this value for each split in which that feature occurs. So in this tree feature 1 has a score of $0.18$, and feature 0 a score of $0.3$.\n",
    "\n",
    "The last step is normalisation. So the final score for features 1 is $\\frac{0.18}{0.18+0.3}=0.325$, and for feature 0, $0.675$.\n",
    "\n",
    "We'll compare this with the scikit learn feature importances for that tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.625 0.375 0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "print(rf.estimators_[0].feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see the results are identical. Worth reiterating the point above- nothing about the above importance calculation depends on Gini specifically, and could be done with any other metric evaluating each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same calculations for the other trees in random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 0 has importances [0.625 0.375 0.    0.    0.   ]\n",
      "Tree 1 has importances [0. 0. 0. 1. 0.]\n",
      "Tree 2 has importances [0. 0. 0. 1. 0.]\n",
      "Tree 3 has importances [0.375 0.    0.    0.625 0.   ]\n",
      "Tree 4 has importances [0.         0.55555556 0.         0.         0.44444444]\n"
     ]
    }
   ],
   "source": [
    "importance_list = []\n",
    "for i, estimator in enumerate(rf.estimators_):\n",
    "    print(\"Tree {} has importances {}\".format(i, estimator.feature_importances_))\n",
    "    importance_list.append(estimator.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is nice and simple, for each feature we take the average of the values for each tree, then will compare with the scikit learn values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2        0.18611111 0.         0.525      0.08888889]\n",
      "[0.2        0.18611111 0.         0.525      0.08888889]\n"
     ]
    }
   ],
   "source": [
    "mean_importance = np.mean(np.asanyarray(importance_list), axis=0)\n",
    "print(mean_importance)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see everything matches up nicely. It's worth mentioning at least one method of feature importance, not used by scikit learn but common for Random Forests, the permutation method. The idea is we permute the values of a given feature within a column, if it's important we will lose predictive power and should see a corresponding difference in the quality of the train model, but if it's not important, we should see less (or possibly no) change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
