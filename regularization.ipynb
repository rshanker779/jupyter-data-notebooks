{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be our input data and $Y$ be our target in a supervised learning problem and condiser $f(x ;  \\theta )$, a class of models we are choosing to solve this problem. \n",
    "\n",
    "For instance we may choose $f$ to be a logistic regression model- the class is the family of models we get for different values of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some error function $E(y_1, y_2)$ which we think of as $E(y, f(x))$ to measure how our model is performing.\n",
    "We then sum up the error for each individaul case to get our total error $$E(Y,f(X, \\theta)) = \\sum_{x \\in X} E(y, f(x, \\theta)) $$\n",
    "\n",
    "\n",
    "Thie error function would typically be mean squared error in a regression scenario, or log loss in a classification scenario. I'll write all this in terms of a generic error function and generic models as the idea of regularization is not model specific, you can basically apply it to any model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we train our model- we use some learning algorithm that finds $\\theta_1, \\theta_2 \\dots $ with the goal being we learn the best $\\theta$, so at some iteration $n$ in the future we have for every $\\theta$ we have $$E(  Y , f(X;\\theta)) \\geq E(Y, f(X;\\theta_n))$$\n",
    "\n",
    "or we have found some $\\theta_n$ that is the best possible set of weights for the problem- that is it is a global minimum over all possible values of $\\theta$. \n",
    "\n",
    "This may or may not be possible depending on the choice of the learning algorithm and our error function $E$- but even if this best case scenario isn't achieved we can often achieve something that's still decent. For instance the error drops below some threshold, or we find a local minimum of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have said nothing interesting, simply described the high level overview. Anyone who has worked with good models an small datasets will know a very common problem is overfitting, where we end up modelling the noise in the dataset as well as the signal. \n",
    "\n",
    "Regularization is a way of limiting our large class of models above to a smaller class of simpler models so hopefully this doesn't occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are loads of ways you could do this of course, but regularization chooses simpler models based on the size of the coefficients- which we will see more formally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note- we will discuss here $L_1$ and $L_2$ regularization, however these are not the only names for these things. You may see LASSO referring to $L_1$ regularization and Ridge referring to $L_2$ especially in statistical journals (or the [Stanford Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about) course). Note also this regularization is performed by default in some of the models we use, for instance XGBoost by default uses $L_2$ as does the Sklearn log reg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $p$ norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a quick diversion to some analysis. I won't be too formal on the underlying set up but given some $n$ dimensional (real valued) vector $$x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\x_n \\end{pmatrix}$$\n",
    "\n",
    "we define a norm for $p>1$ $$\\Vert{x}\\Vert_p = \\left( \\sum_{i=1}^n \\vert x_i \\vert ^ p \\right) ^{1/p}$$\n",
    "\n",
    "You can check this is actually a norm if you're feeling mathematically inclined (note triangle inequlity is not trivial), however there's no need as we generally only work with two of these norms where $p=1$ and $p=2$, which we call the $L_1$ and $L_2$ norms respectively (if you care about why we use $L$ it's because  of the [$L_p$ space](https://en.wikipedia.org/wiki/Lp_space) but this will likely add nothing to your life).\n",
    "\n",
    "Let's have a look at these in some more detail- we'll start with the easy one, $L_2$ $$\\Vert{x}\\Vert_2 = \\left( \\sum_{i=1}^n \\vert x_i \\vert ^ 2 \\right) ^{1/2} = \\sqrt{x_1^2 + x_2^2 + \\dots +x_n^2}$$\n",
    "\n",
    "Hopefully this should look very familiar- it's the standard Euclidean norm, or Pythagoras' Theorem in the case $n=2$, the standard way we all measure distance.\n",
    "\n",
    "The other norm is a bit more subtle even though it has a simpler formula $$\\Vert{x}\\Vert_1 =  \\sum_{i=1}^n \\vert x_i \\vert  = \\vert x_1 \\vert + \\vert x_2 \\vert  + \\dots + \\vert x_n \\vert $$\n",
    "\n",
    "What this is saying is we measure distance by just taking sum of how far we are away in each co-ordinate independently. This norm is also called the [taxicab norm](https://en.wikipedia.org/wiki/Taxicab_geometry).\n",
    "\n",
    "The rough intuition of $p$ norms is smoothness- the higher $p$ the smoother the norm is. A picture will show this a bit more clearly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_1 = np.linspace(-1,1)\n",
    "f_1 = lambda x: 1 - x if x >= 0 else 1 + x\n",
    "f_2 = lambda x: x - 1 if x >= 0 else - x - 1\n",
    "g_1 = lambda x: sqrt(1-x**2)\n",
    "g_2 = lambda x: -sqrt(1-x**2)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot(x_1, np.vectorize(f_1)(x_1), color = 'blue')\n",
    "axs[0].plot(x_1, np.vectorize(f_2)(x_1), color = 'blue')\n",
    "axs[0].plot(2*x_1, 0 * x_1, 'black')\n",
    "axs[0].plot(0*x_1, 2 * x_1, 'black')\n",
    "axs[0].set_aspect('equal')\n",
    "axs[0].set_title('Unit circle taxicab norm')\n",
    "axs[1].plot(x_1, np.vectorize(g_1)(x_1), color = 'blue')\n",
    "axs[1].plot(x_1, np.vectorize(g_2)(x_1), color = 'blue')\n",
    "axs[1].plot(2*x_1, 0 * x_1, 'black')\n",
    "axs[1].plot(0*x_1, 2 * x_1, 'black')\n",
    "axs[1].set_aspect('equal')\n",
    "axs[1].set_title('Unit circle euclidean norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above I have plotted two unit circles for the $L_1$ and $L_2$ norms in 2 dimensions. The unit cricle is the set of all points which are of distance 1 from the origin where the distance is measure by norm. So as we would expect the Euclidean unit circle looks like a normal circle, whereas in the $L_1$ norm it is a square.\n",
    "\n",
    "This is the difference in \"smoothness\" between the norms. If we also have a look at the $L_3$ norm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAD8CAYAAACPd+p5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAF8RJREFUeJzt3XmUFOW5x/HvAwyLIm4MIgLigguKooygRw24JeBRUWMU90QRTCQRzeJ29Hpc4pJE0cTlEvEoMQm4RqN4jQvEHQXFBccFNOoIkQEEURQY5rl/vD1xgt1dA9XdVd3z+5xTp6d7iq6nYeZHvVXvYu6OiEg+bZIuQETST0EhIpEUFCISSUEhIpEUFCISSUEhIpEUFCISSUEhIpEUFCISqV3SBeTTtWtX79OnT9JliFSsWbNmLXL36qj9Uh0Uffr0YebMmUmXIVKxzOzDluynpoeIRFJQiEgkBYWIRFJQiEgkBYWIRIodFGbWy8ymmVmtmc0xs7Oz7GNmdqOZzTWz181sz7jHFZHSKcTt0Qbg5+7+ipltBMwys8fd/a1m+wwH+ma2wcAtmUcRKQOxzyjcfYG7v5L5ejlQC2y11m4jgEkevAhsYmZbxj22pMe4ceMYN25c0mVIkRS0w5WZ9QH2AGas9a2tgI+bPa/LvLYgy3uMBkYD9O7du5DlSRHNnj076RKkiAp2MdPMOgP3AePc/fO1v53lj2Sd1dfdJ7h7jbvXVFdH9iwVkRIoSFCYWRUhJP7s7vdn2aUO6NXseU9gfiGOLSLFV4i7HgZMBGrd/bocuz0EnJK5+7E3sMzdv9XsEJF0KsQ1in2Bk4E3zKypoXoh0BvA3W8FpgKHAnOBFcCPCnBcESmR2EHh7s+S/RpE830cOCvusUQkGeqZKSKRFBQiEklBISKRFBQiEklBISKRFBQiEklBISKRFBQiEklBISKRFBQiEklBISKRFBQiEklBISKRFBQiEklBISKRCjUV3u1mttDM3szx/aFmtszMZme2SwpxXBEpjULNwn0H8AdgUp59nnH3wwp0PBEpoYKcUbj708CSQryXiKRPKa9R7GNmr5nZo2a2SwmPKyIxFXQBoDxeAbZ29y/M7FDgb4TlBb9FCwCJpE9Jzijc/XN3/yLz9VSgysy65thXCwCJpExJgsLMumfW/8DMBmWOu7gUxxaR+ArS9DCzvwJDga5mVgf8D1AF/1nX4xjgx2bWAHwFjMxM4S8iZaAgQeHux0d8/w+E26ciUobUM1NEIikoRCSSgkJEIikoRCSSgkJEIikoRCSSgkJEIikoRCSSgkJEIikoRCSSgkJEIikoRCSSgkJEIikoRCSSgkJEIikoRCRSqRYAMjO70czmmtnrZrZnIY4rIqVRqDOKO4Bheb4/nDDrdl/CDNu3FOi4IlIChZoK72kz65NnlxHApMw8mS+a2SZmtqW7LyjE8aX43GH1ali1ChoaYM0aaGwMG4TXAT79NDyaQZs2YWvXLmwdOkDbtsnUL/GUal2PrYCPmz2vy7ymoEjQypUwd27YPvgA6upgwQJYuBCWLIGlS+Hzz2HFCvjqqxAWUbp3z//9du1ggw2gc2fo0gU23RS6doVu3aBnz7Bttx3ssAP06BECR5JXqqDI9s+d9cdOCwAVR2MjvPYaPPMMzJgBr74K774bzgyadOwIW24Zfmm7d4edd4aNNoINNwy/3B06QPv24Ze9bduwmYXt+uvDe5xzzjfHcw/vv2ZNOBtZuRK+/joEzxdfwLJl8Nln8NFH8NJLIaCah9HGG0P//rDXXrD33jBkCGyxRen+zuQbpQqKOqBXs+c9gfnZdnT3CcAEgJqaGk3pH8MXX8DUqfC3v8Fjj4WzBAj/U9fUwPe/H8Kgb1/YZhvYfPP1/x98ypTw+OMfr3+9q1bBJ5/AvHkhxN58M4TbLbd8E0S77gqHHw5HHRU+g844SqNUQfEQMNbMJgODgWW6PlEc7vDcc/DHP8K994b/vaurwy/XwQfD0KHh9D6N2rcPgbXNNqHWJqtXw+zZ8NRTIfCuvRauuio0T049FUaNCmdBUkTuHnsD/kq43rCacPZwOnAmcGbm+wbcBMwD3gBqWvK+AwcOdGmZxkb3++5z32svd3Dv0sV99Gj3f/7TvaGh+McfMmSIDxkypPgHcvfFi90nTnT/znfCZ+3QwX3UKPd580py+IoCzPSW/I63ZKekNgVFy0yf7l5TE/41+/Z1v+UW9y+/LG0NpQyK5mpr3c88071jR/eqKvexY90XLSp5GWWrpUGhnpllbPFiOO200JxYuBDuuANqa+HMM8PFx9Zgp53CNYx58+D008PXO+0Ed93Vsrs00jIKijL1zDOw227wpz/BBRfA22+H9npr7afQo0cIiVdfhe23h5NPhuOOC7d3JT4FRZlxD3cADjggnDW89BL8+tfQqVPSlaVD//7w7LNwzTVw//0wcGA4y5J4FBRlpLERfvlLOPdcGDECZs2CPfZIuqr0adsWfvUrmDYNli+H/fYLfUdk/SkoyoQ7jBkDv/sdjB0L99wTejZKbvvvD88/H3p/HnggPP100hWVLwVFmbj4YrjtNrjoIrjxxjCGQqJtu21oimy9dTgLmzMn6YrKk37cysCECXDllXDGGXD55eqNuK66d4dHHw3XcYYNg/lZ+wRLPgqKlHv9dfjpT2H4cLj5ZoXE+tp669Cd/bPP4JRTvhn1Ki2joEixr7+GE0+EzTaDSZPCYCxZfwMGhDtGTz4Zmm/ScgqKFLv44jAw6vbbw1BsiW/UKDjiCDj/fHjrraSrKR8KipR6/30YPz78YA8fnnQ1lcMsDJjr1AnOOy/pasqHgiKlLr0UqqrgssuSrqTydOsW+lk8/DC88ELS1ZQHBUUKzZkTxiqMHRsmkpHC+9nPQmBcdFHSlZQHBUUKXXppmCpOp8bFs+GGcOGFoffmtGlJV5N+CoqUWbQozEg1ZkyYcUqKZ8yYcEdp4sSkK0k/BUXK3H9/mOX6xBOTrqTydewYpgN88MEwE5jkVqgFgIaZ2TuZBX7Oz/L9H5pZvZnNzmyjCnHcSjR5Muy4I+y+e9KVtA4jR34zt6jkFjsozKwtYZq74UA/4Hgz65dl1ynuPiCz3Rb3uJVowQKYPj388KoHZmk0zew9eXLSlaRbIc4oBgFz3f19d18FTCYs+CPr6J57wijR445LupLWo21bOPZYeOQRTXKTTyGCItfiPmv7fmbd0XvNrFeW7wNhXQ8zm2lmM+vr6wtQXvm4557Q5Nh556QraV1Gjgzd5R9+OOlK0qsQQdGSxX3+DvRx992AJ4A7c72Zu09w9xp3r6muri5AeeXjnXfCQjdSWoMGhcd33022jjQrRFBELu7j7ovdfWXm6R+BgQU4bkVZsybcGm1l2ZgK7dqF26QLFyZdSXoVIiheBvqa2TZm1h4YSVjw5z/MrHn/wiMAzWK4liVLwvUJLWSTjG7doJW1dNdJ7IHL7t5gZmOBx4C2wO3uPsfMLiOsGfAQ8DMzOwJoAJYAP4x73ErT9EOqM4pkVFcrKPIpyAwH7j4VmLrWa5c0+/oC4IJCHKtSNZ32KiiSUV2t2brzUc/MlGj630xNj2So6ZGfgiIl1PRIVnV1WHltzZqkK0knBUVKNDU9NBAsGdXV4WLy4sVJV5JOCoqUqK8Pt+iqqpKupHVqavKp+ZGdgiIl6uvV7EhS09+9giI7BUVKLFyooEhS09+9Ol1lp6BIifp63fFIkpoe+SkoUkJNj2Q1XURWUGSnoEgBjfNInsZ75KegSAGN80gHdbrKTUGRAupslQ4a75GbgiIFNM4jHaqr1fTIRUGRAhrnkQ5qeuSmoEgBNT3SQeM9clNQpIDGeaSDxnvkVqp1PTqY2ZTM92eYWZ9CHLdSaJxHOqjTVW6lWtfjdOAzd98euB64Ju5xK4k6W6WDxnvkVqp1PUbwzczb9wIHmWmJmyYa55EOGu+RWyGmwsu2rsfgXPtk5thcBmwOLIpz4HHjxjF79uw4b5EKzz0HHTrA0KFJV7L+mv4dhpbxh1i1KjyOGwc335xsLXENGDCA8ePHF+z9SrWuR0v2CTu2wgWAGhvDJunQ0JB0BelTiDOKyHU9mu1TZ2btgI0Js3F/i7tPACYA1NTUZA2TJoVMzCTtv38YazBtWtKVrL+mM4np06cnWkcctbXQrx/ccAMcf3zS1aRLSdb1yDw/NfP1McBT7p43BFoT9QhMB/WQzS12ULh7A9C0rkctcHfTuh6ZtTwAJgKbm9lc4FzgW7dQWzP1CEwH9ZDNrVTrenwN/KAQx6pEzXsEtm2bdDWtl3rI5qaemSlQXR0uZi7JetVGSqWp6dG1a7J1pJGCIgXUIzAd6uth003VQzYbBUUKqEdgOqiHbG4KihRQj8B0UA/Z3BQUKaCmRzpoJvTcFBQpoBmg00FNj9wUFClQVRUuoqnpkZzGRs2Eno+CIiXU6SpZS5aEsFDTIzsFRUpoBuhkqbNVfgqKlNB4j2RpnEd+CoqUUNMjWRrnkZ+CIiU0A3Sy1PTIT0GREhrvkSyN88hPQZES6sadrPp62GQTjfPIRUGREuqdmSz1ysxPQZESOqNIlnpl5hcrKMxsMzN73MzeyzxummO/NWY2O7OtPU2eoIFhSdOAsPzinlGcDzzp7n2BJ8k9xd1X7j4gsx2RY59Wrekims4okqGmR35xg6L5wj53AkfGfL9Wq2m8h4Ki9DTOI1rcoNjC3RcAZB5zZXLHzFodL5qZwiSHPn3gtdeSrqL1mTMnhEWfPklXkl6Rk+ua2RNA9yzfumgdjtPb3eeb2bbAU2b2hrvPy3G80cBogN69e6/DIcrf0UfDxRdDXR307Jl0Na3HlCnQpg0coUZxTpFnFO5+sLvvmmV7EPjUzLYEyDxmvRTn7vMzj+8D04E98hxvgrvXuHtNdSs7FzzuuPB4993J1tGauMPkyXDQQbpGkU/cpkfzhX1OBR5cewcz29TMOmS+7grsC7wV87gVqW9fGDgw/OBKacyaBfPmwciRSVeSbnGD4mrgEDN7Dzgk8xwzqzGz2zL77AzMNLPXgGnA1e6uoMhh5Eh4+eXwwyvFN3lyuJB81FFJV5JusYLC3Re7+0Hu3jfzuCTz+kx3H5X5+nl37+/uu2ceJxai8Ep17LHhUWcVxdfYGK5PDBsW7jhJbuqZmTK9e8OBB8JNN8GKFUlXU9mmTAkXjk85JelK0k9BkUKXXgoLFoSwkOJYvRouuQR22y3cbZL8FBQptP/+8L3vwdVXw+efJ11NZbrzTpg7Fy6/PNwalfz0V5RSV14Z5qb47W+TrqTyrFgBl10GgwfD4YcnXU15UFCk1MCBcMIJcNVV8MorSVdTWX7xC/j4Y7j2WjBLupryoKBIsd//PnQCOukk+OqrpKupDI8+CrfcAueeC9/5TtLVlA8FRYptthnccQfU1sK4caEXoay/Tz6B006DXXcNTTtpOQVFyh1yCJx/PkyYAL/5TdLVlK9ly2D4cPjyS/jLX6Bjx6QrKi+Rg8IkeVdeCf/6F5x3Hmy5JZx8ctIVlZevvgq3QGtrQ9Ojf/+kKyo/Cooy0KZNaILU18Opp8Ly5fCTnyRdVXlYujSMCn32WZg0CQ4+OOmKypOaHmWiQwf4+9/D7byzzoILL9QaIFE++ACGDIEXXwxd4k86KemKypeCoox06gT33QdnnBFum373u/DvfyddVTo99BDsuSd8+CE88sg3Y2hk/Sgoyky7duHC5u23wwsvhCv4kybpjkiTRYtC82zECNhuu9AH5ZBDkq6q/CkoytSPfgQzZ8KOO4ZfjKFDwyl2a7VyJYwfDzvtFO5qXHQRPPccbLtt0pVVBgVFGevXD555Bm69Fd5+G/bZBw47DKZPbz1nGMuXww03wA47wDnnwIAB8OqrcMUV4bqOFIaCosy1aQNjxoSJbq64AmbMgAMOgN13h+uvr8xrGI2N8Pzz4XP37Bk6o/XqBY8/Dk88EZpjUlhxFwD6gZnNMbNGM6vJs98wM3vHzOaaWa61PySGzp3D6fZHH4VrGJ06hW7KPXrAfvvBNdeEpkq53ilZuhQefDDc8endG/bdF+66K1yLmDEj3P7Urc/iiduP4k3gaOB/c+1gZm2BmwhT5dUBL5vZQ5oOrzg6dQp3Rc44A956C+69Fx54IPTuBNhoI9hrr7D17x+aL9ttB126JFt3k8ZGmD8f3nsP3ngjLF/w0kvhszQ2wgYbhCH4Rx0FRx4ZPo8UX6ygcPdaAMs/BG8QMDczAzdmNpmwcJCCosj69QuTs1xySWiCPPVU+J93xgy47roweUuTzTcPp+89eoSFcLp2Dat7d+kSfjk32CC0+du3D3de2rULzR6zsC1dGt5n2rTw6B5+sdesCdvq1eGC48qVoRv1F1+EbtVLl4aOZJ9+GmabqquDr7/+77oGDYJjjglNqsGDde0hCaXombkV8HGz53XA4Fw7t+Z1PYqpe/cwbP2EE8Lz1avh3XdDt+Z580LnpLq68L/566/D4sXrN2L1wANbvq8ZbLxxCKZu3cLQ+iOPDGc4228Pu+wS6tZQ8OTFWgAos7ZH5FtkeS3nNXl3nwBMAKipqWkl1+5Lr6oq/CLuskvufVavDjNsrVgRtpUrYdUqaGgIW9NZA8DZZ4fHG2745s+3bRt+yauqwhlIhw5h23DDsHXurNmlykVkULh73EtEdUCvZs97AvNjvqeUQFVVOPXffPPofTfZJDwOGVLcmiQZpcjzl4G+ZraNmbUHRhIWDhKRMhH39uhRZlYH7AM8YmaPZV7vYWZTAdy9ARgLPAbUAne7+5x4ZYtIKcW96/EA8ECW1+cDhzZ7PhWYGudYIpIcXUoSkUgKChGJpKAQkUgKChGJpKAQkUgKChGJpKAQkUgKChGJpKAQkUgKChGJpKAQkUgKChGJpKAQkUgKChGJpKAQkUgKChGJVKoFgP5lZm+Y2WwzmxnnmCJSekVfAKiZA9x9UczjiUgCSrEAkIiUuVJdo3DgH2Y2K7PAT05mNtrMZprZzPr6+hKVJyL5lGIBIIB93X2+mXUDHjezt9396Ww7agEgkfQpxQJATbNy4+4LzewBwnqkWYNCRNKn6E0PM9vQzDZq+hr4LuEiqIiUiaIvAARsATxrZq8BLwGPuPv/xTmuiJRW0RcAcvf3gd3jHEdEkqWemSISSUEhIpEUFCISSUEhIpEUFCISSUEhIpEUFCISSUEhIpEUFCISSUEhIpEUFCISSUEhIpEUFCISSUEhIpEUFCISKe7ENb8xs7fN7HUze8DMNsmx3zAze8fM5prZ+XGOKSKlF/eM4nFgV3ffDXgXuGDtHcysLXATMBzoBxxvZv1iHldESihWULj7P9y9IfP0RaBnlt0GAXPd/X13XwVMBkbEOa6IlFYhr1GcBjya5fWtgI+bPa/LvCYiZaIg63qY2UVAA/DnbG+R5bWc63VkFggaDdC7d++o8iQlBgwYkHQJUkSx1/Uws1OBw4CD3D1bANQBvZo97wnMz3M8LQBUhsaPH590CVJEce96DAPOA45w9xU5dnsZ6Gtm25hZe2Ak8FCc44pIacW9RvEHYCPCMoGzzexW+O91PTIXO8cCjwG1wN3uPifmcUWkhOKu67F9jtf/s65H5vlUYGq2fUUk/dQzU0QiKShEJJKCQkQiKShEJJKCQkQiWfY+UulgZvXAhxG7dQUWlaCcYquEz1EJnwEq43O09DNs7e7VUTulOihawsxmuntN0nXEVQmfoxI+A1TG5yj0Z1DTQ0QiKShEJFIlBMWEpAsokEr4HJXwGaAyPkdBP0PZX6MQkeKrhDMKESmyigiKlk7ym3Zm9gMzm2NmjWZWVlfdK2ECZTO73cwWmtmbSdeyvsysl5lNM7PazM/S2YV434oIClowyW+ZeBM4Gng66ULWRQVNoHwHMCzpImJqAH7u7jsDewNnFeLfoiKCooWT/Kaeu9e6+ztJ17EeKmICZXd/GliSdB1xuPsCd38l8/VywhwwseeorYigWEuuSX6leDSBcgqZWR9gD2BG3PeKNXFNKRVgkt9UaMnnKEPrNIGyFJ+ZdQbuA8a5++dx369sgqIAk/ymQtTnKFPrNIGyFJeZVRFC4s/ufn8h3rMimh4tnORXikcTKKeEmRkwEah19+sK9b4VERTkmOS33JjZUWZWB+wDPGJmjyVdU0tUygTKZvZX4AVgRzOrM7PTk65pPewLnAwcmPldmG1mh0b9oSjqmSkikSrljEJEikhBISKRFBQiEklBISKRFBQiEklBISKRFBQiEklBISKR/h/u1gdusol8gAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_1 = np.linspace(-1,1)\n",
    "f_1 = lambda x: 1 - x if x >= 0 else 1 + x\n",
    "f_2 = lambda x: x - 1 if x >= 0 else - x - 1\n",
    "g_1 = lambda x: (1-x**3)**(1.0/3) if x>0 else (1+x**3)**(1.0/3)\n",
    "g_2 = lambda x: -(1-x**3)**(1.0/3) if x>0 else -(1+x**3)**(1.0/3)\n",
    "\n",
    "plt.plot(x_1, np.vectorize(g_1)(x_1), color = 'blue')\n",
    "plt.plot(x_1, np.vectorize(g_2)(x_1), color = 'blue')\n",
    "plt.plot(2*x_1, 0 * x_1, 'black')\n",
    "plt.plot(0*x_1, 2 * x_1, 'black')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a \"smoother\" circle. We now have the tools to go back and define more carefully what we mean by making coefficients smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can know set up a more formal optimization problem. You can view the idea of training a model as trying to solve this problem: $$ \\min_\\theta E(Y, f(X;\\theta)) $$\n",
    "\n",
    "which is exactly as described earlier- trying to find the global minimum (or something close) across all such $\\theta$. Regularization simply takes this existing problem and applies a penalty for the size of the coefficients\n",
    "$$\\min_\\theta E(Y, f(X;\\theta)) \\quad\\text{subject to}\\quad \\Vert \\theta \\Vert_p \\leq C $$\n",
    "\n",
    "Any $p$ (or indeed any norm) would work here, but we are basically considering $p=1$ or $p=2$. This parameter $C$ must be chosen by us (through cross validation) and can be thought of as a budget. When $C$ is very high we have a lot of flexibility in the coefficients, so a more complicated model. Conversely when $C$ is low we have a simpler model.\n",
    "\n",
    "We can also reframe this in the language of Lagrange multipliers too $$\\min_\\theta E(Y, f(X;\\theta)) + \\lambda \\Vert \\theta \\Vert_p  $$\n",
    "\n",
    "with $\\lambda$ as our parameter (which is now an inverse budget).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between $L_1$ and $L_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a two ways we can apply a penalty to our coefficients and they look very similar. Both prevent any single coefficient getting too large, however there is a clear difference in how they behave with multiple parameters.\n",
    "\n",
    "The $L_1$ penalty produces sparse models i.e. models where many of the coefficients are 0, whereas the $L_2$ penalty does not. In fact the $L_2$ model often shrinks coefficients so they are very small, but it is unlikely they ever disappear.\n",
    "\n",
    "So why does this happen? Here is the image that shows why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://qph.fs.quoracdn.net/main-qimg-583e319a860c9cc4a187e005fb4b7353\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://qph.fs.quoracdn.net/main-qimg-583e319a860c9cc4a187e005fb4b7353')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the picture showing? For $L_1$ the squareness of the unit circle means solutions to the constraint on coefficients are more likely to hit the corners, and hitting the corners is precisely when coefficients vanish. On the other hand in $L_2$, solutions can hit everywhere with equal probability, meaning it is very unlikely for coefficients to exactly vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the picture in a bit more detail, let's flesh out exactly what it is showing with an example. \n",
    "\n",
    "Let's consider two independent variables $x_1$ and $x_2$, and a training set $(1, 0) $ corresponding to a target value of $a$, and $(0,1)$ corresponding to a target value of $b$ where $a,b>0$. \n",
    "\n",
    "We will choose a linear model to map this, so our model family is $$f(x_1, x_2;\\theta) = x_1\\theta_1 + x_2\\theta_2$$ where $\\theta =(\\theta_1, \\theta_2)$ are our weights and for simplicity we are assuming no intercept term. For our error function we will choose mean squared error.\n",
    "\n",
    "So now our prediction for our first training case is $f(1,0)=a$ and for our second $f(0,1)=b$. If we had no regularization constraint we could set $\\theta_1=a$ and $\\theta_2 =b$ and model this perfectly- but what happens when we impose a constraint?\n",
    "\n",
    "With the $L_1$ penalty and a budget of 1 we will try and minimize $$\\min_{\\theta} (a-\\theta_1)^2 + (b-\\theta_2)^2 \\quad \\text{subject to} \\quad \\vert\\theta_1\\vert+\\vert\\theta_2\\vert=1 $$\n",
    "\n",
    "This is exactly what the above picture shows. We want to minimize the distance between the point $(a,b)$ and $(\\theta_1, \\theta_2)$ subject to $(\\theta_1, \\theta_2)$ lying inside the unit circle for the $L_1$ norm. The $L_2$ situation is the same, just with a different unit circle.\n",
    "\n",
    "Now you can either draw a few pictures or solve some basic equations and you should see that with the $L_2$ unless $a=0$ or $b=0$ then $\\theta_1 \\neq 0 $ and $\\theta_2 \\neq 0$. So the only way for coefficient to vanish is if the training cases lie on an axis\n",
    "\n",
    "However for the $L_1$ case, unless $ a+1>b>a-1$ a coefficient will vanish (it is worth trying to show this yourself). So basically there are large areas of the plane where only a sparse model will be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a toy example, but it illustrates the principle quite nicely, and the same thing happens for other models and more complicate training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What this means in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a brief intro to what regularization means and how it works. As noted above the models we use in prod have regularization built in, and for our purposes we don't really need to consider the exact method. Both $L_1$ and $L_2$ regularization have some advantages and disadvantages, so there is no correct one to choose for all problems. In addition the elastic net (basically a combo of both $L_1$ and $L_2$ regularization) is a more flexible approach that allows you to get some benefits of both methods. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
